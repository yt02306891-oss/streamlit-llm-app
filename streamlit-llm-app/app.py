import os
import streamlit as st
from dotenv import load_dotenv  # â† è¿½åŠ 

# LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# =============== .env ã®èª­ã¿è¾¼ã¿ ===============
# .env ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œã—ã¦ãã ã•ã„
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # â† .env ã‹ã‚‰å–å¾—

st.set_page_config(page_title="Expert Chat (LangChain + Streamlit)", page_icon="ğŸ’¬", layout="centered")
st.title("Expert Chatï¼ˆLangChain + Streamlitï¼‰")
st.caption(
    "å…¥åŠ›æ¬„ã«ç›¸è«‡ã‚„è³ªå•ã‚’æ›¸ãã€ä¸‹ã®ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"
    "é€ä¿¡ã™ã‚‹ã¨ã€é¸ã‚“ã å°‚é–€å®¶ã¨ã—ã¦ã®å£èª¿ãƒ»è¦³ç‚¹ã§å›ç­”ã—ã¾ã™ã€‚"
)

with st.expander("ã“ã®Webã‚¢ãƒ—ãƒªã®æ¦‚è¦ãƒ»æ“ä½œæ–¹æ³•", expanded=True):
    st.markdown(
        """
**æ¦‚è¦**  
- å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆè³ªå•ãƒ»ç›¸è«‡ï¼‰ã‚’ LangChain çµŒç”±ã§ LLM ã«æ¸¡ã—ã€å›ç­”ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚  
- ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ **å°‚é–€å®¶ã®ç¨®é¡** ã‚’é¸ã¶ã¨ã€**ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**ãŒåˆ‡ã‚Šæ›¿ã‚ã‚Šã€LLM ãŒãã®å°‚é–€å®¶ã¨ã—ã¦æŒ¯ã‚‹èˆã„ã¾ã™ã€‚

**æ“ä½œæ‰‹é †**  
1. å…¥åŠ›æ¬„ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¾ã™  
2. ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã³ã¾ã™  
3. **é€ä¿¡** ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€LLM ã®å›ç­”ãŒä¸‹ã«è¡¨ç¤ºã•ã‚Œã¾ã™
        """
    )

# =============== å°‚é–€å®¶ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆA/Bï¼‰ ===============
EXPERT_MAP = {
    "Aï¼šè‡¨åºŠå¿ƒç†å£«ï¼ˆãƒ¡ãƒ³ã‚¿ãƒ«ã‚±ã‚¢ã®å°‚é–€å®¶ï¼‰": {
        "system": (
            "ã‚ãªãŸã¯è‡¨åºŠå¿ƒç†å£«ã¨ã—ã¦å¯¾è©±ã—ã¾ã™ã€‚æ¥è«‡è€…ã®å®‰å…¨ã¨å°Šé‡ã‚’æœ€å„ªå…ˆã—ã€"
            "å…±æ„Ÿçš„ãƒ»éè©•ä¾¡çš„ã«ã€èªçŸ¥è¡Œå‹•ç™‚æ³•ã‚„ã‚¹ãƒˆãƒ¬ã‚¹å¯¾å‡¦ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’è¸ã¾ãˆã€"
            "å®Ÿè·µå¯èƒ½ãªå°ã•ãªè¡Œå‹•ææ¡ˆã‚’å…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„ã€‚éåº¦ãªåŒ»ç™‚åˆ¤æ–­ã¯é¿ã‘ã€"
            "å¿…è¦ã«å¿œã˜ã¦å°‚é–€æ©Ÿé–¢ã®å—è¨ºå‹§å¥¨ã‚‚è¡Œã„ã¾ã™ã€‚"
        )
    },
    "Bï¼šçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆï¼ˆæˆ¦ç•¥ã¨å®Ÿè¡Œã®å°‚é–€å®¶ï¼‰": {
        "system": (
            "ã‚ãªãŸã¯çµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã¨ã—ã¦å¯¾è©±ã—ã¾ã™ã€‚è«–ç‚¹ã‚’ç´ æ—©ãæ•´ç†ã—ã€"
            "ç›®çš„ãƒ»KPIãƒ»ä»£æ›¿æ¡ˆãƒ»ãƒªã‚¹ã‚¯ãƒ»å®Ÿè¡Œè¨ˆç”»ã‚’ç°¡æ½”ã«æç¤ºã—ã¦ãã ã•ã„ã€‚"
            "å¯èƒ½ã§ã‚ã‚Œã°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆ3C/4P/MECEç­‰ï¼‰ã‚’ç¤ºã—ã€æ¬¡ã®ä¸€æ‰‹ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚"
        )
    },
}

def get_llm_response(input_text: str, expert_key: str) -> str:
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶é¸æŠã«å¿œã˜ã¦ LLM å¿œç­”ã‚’è¿”ã™"""
    system_message = EXPERT_MAP[expert_key]["system"]

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_message),
            ("human", "{question}"),
        ]
    )

    # OPENAI_API_KEY ã¯ .env ã‹ã‚‰èª­ã¿è¾¼ã¿æ¸ˆã¿ï¼ˆlangchain_openaiã¯ç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ï¼‰
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆ.env ã‚’ç¢ºèªï¼‰")

    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY  # å¿µã®ãŸã‚æ˜ç¤ºã‚»ãƒƒãƒˆ
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    chain = prompt | llm
    result = chain.invoke({"question": input_text})
    return result.content

# ===== UI =====
with st.form("chat_form", clear_on_submit=False):
    user_input = st.text_area(
        "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ",
        placeholder="ä¾‹ï¼‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²ã‚æ–¹ã§æ‚©ã‚“ã§ã„ã¾ã™ã€‚ãƒãƒ¼ãƒ ã‚’ã†ã¾ãå‹•ã‹ã™ã‚³ãƒ„ã¯ï¼Ÿ",
        height=140,
    )
    expert_choice = st.radio(
        "å°‚é–€å®¶ã‚’é¸æŠ",
        options=list(EXPERT_MAP.keys()),
        index=0,
    )
    submitted = st.form_submit_button("é€ä¿¡")

if submitted:
    try:
        if not user_input or not user_input.strip():
            st.warning("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("LLMã«å•ã„åˆã‚ã›ä¸­..."):
                answer = get_llm_response(user_input.strip(), expert_choice)
            st.success("å›ç­”ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            st.markdown("###  å›ç­”")
            st.write(answer)
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")